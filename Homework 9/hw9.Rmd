---
title: Problem Set 9
author: Ansel George
output: pdf_document
---

```{r,message=F}
library(ggplot2)
library(MASS)
library(mvtnorm)

library(dplyr)
library(tidyr)

set.seed(10)
```


# Brownian Motion, Gaussian processes

## A: Brownian Motion

* Let $X(t)$ be conducting standard Brownian motion with $X(0)=0$. Write code
  to simulate $X(t_1),\hdots,X(t_k)$ for any specified vector
  $\textbf{t}=(t_1,\hdots,t_k)$ (all positive, and increasing). [Hint: you
  might want to exploit the fact that the $X$'s form a non-homogeneous Markov
  chain.] Show a plot ($t$ vs $X(t)$) of an example simulation for
  $\textbf{t}=(t_1,\hdots.,t_{1000})$ where the $t_k$ are the order statistics
  of $1000$ independent and identically distributed (iid) uniform samples on
  $[0,1]$.

```{r}
walkRandomly <- function(steptimes, x0, u=0, v=1) {
  x <- rep(0, length(steptimes)+1)
  stepsizes <- rep(0, length(steptimes))

  x[1] <- x0
  stepsizes[1] <- rnorm(1, u, sqrt(steptimes[1]))
  x[2] <- stepsizes[1] + x[1]
  for (i in 2:length(steptimes)) {
    stepsizes[i] <- rnorm(1, u, sqrt(v*(steptimes[i]-steptimes[i-1])))
    x[i+1] <- stepsizes[i] + x[i]
  }

  return(list(pos=x,sizes=stepsizes))
}
```

```{r}
x0 <- 0
nsteps <- 1000

steptimes <- sort(runif(nsteps))
res <- walkRandomly(steptimes, x0)
walk <- res$pos
stepsizes <- res$sizes
qplot(c(0,steptimes), walk, geom='line')
```

* Let $X(t)$ be conducting scaled Brownian motion with variance parameter
  $\sigma^2$ (so $X(t)-X(s) | X(s) \sim N(0,\sigma^2 (t-s))$). Find an
  expression for the log-likelihood for $\sigma$ given data
  $X(t_1),\hdots,X(t_k)$ for any specified vector $t=(t_1,\hdots,t_k)$ (all
  positive, and increasing). Use this to compute and plot the log-likelihood
  $l(\sigma)$, as a function of $\sigma$, for your example simulated data
  above. What is the maximum likelihood estimate for $\sigma$? [it suffices to
  read this from your plot, though an analytic solution is also easily
  available.]

\begin{align}
L(\sigma^2) &= P(X(t_1), \dots, X(t_n) | \sigma^2) \\
  &= P(Z_{t_1}, \dots, Z_{t_n} | \sigma^2) \\
  &= P(Z_{t_1})P(Z_{t_2})\dots P(Z_{t_n}) \\
\implies \ell(\sigma^2) &= \sum_{i=1}^n \log{P(Z_{t_i})} \\
  &= \sum_{i=1}^n \log{\frac{1}{\sqrt{2 \pi \sigma^2(t_{i} - t_{i-1})^2}} e^{-\frac{x_i^2}{2\sigma^2(t_{i} - t_{i-1})^2}}}\\
  &= \sum_{i=1}^n \log{\frac{1}{\sqrt{2 \pi \sigma^2(t_{i} - t_{i-1})^2}}} - \sum_{i=1}^n \frac{x_i^2}{2\sigma^2(t_{i} - t_{i-1})^2} \\
  &= \sum_{i=1}^n \log{\frac{1}{\sqrt{2 \pi(t_{i} - t_{i-1})^2}}} + \sum_{i=1}^n \log{\frac{1}{\sqrt{\sigma^2}}} - \sum_{i=1}^n \frac{x_i^2}{2\sigma^2(t_{i} - t_{i-1})^2} \\
  &= \sum_{i=1}^n \log{\frac{1}{\sqrt{2 \pi(t_{i} - t_{i-1})^2}}} + n \log{\frac{1}{\sqrt{\sigma^2}}} - \frac{1}{\sigma^2} \sum_{i=1}^n \frac{x_i^2}{2(t_{i} - t_{i-1})^2} \\
\implies \frac{\partial \ell}{\partial(\sigma^2)} &= - \frac{n}{2 \sigma^2} + \frac{1}{\sigma^4} \sum_{i=1}^n \frac{x_i^2}{2(t_{i} - t_{i-1})^2} \\
\end{align}

The MLE therefore:
\begin{align}
0 &= - \frac{n}{2 \sigma^2} + \frac{1}{\sigma^4} \sum_{i=1}^n \frac{x_i^2}{2(t_{i} - t_{i-1})^2} \\
0 &= - \frac{n}{\sigma^2} + \frac{1}{\sigma^4} \sum_{i=1}^n \frac{x_i^2}{(t_{i} - t_{i-1})^2} \\
\implies \frac{n}{\sigma^2} &= \frac{1}{\sigma^4} \sum_{i=1}^n \frac{x_i^2}{(t_{i} - t_{i-1})^2} \\
\implies n \sigma^2 &= \sum_{i=1}^n \frac{x_i^2}{(t_{i} - t_{i-1})^2} \\
\implies \hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^n \frac{x_i^2}{(t_{i} - t_{i-1})^2} \\
\end{align}

```{r}
likRandomly <- function(steptimes, stepsizes, u=0, v=1) {
  timesteps<-c(steptimes[1], steptimes[2:length(steptimes)] - steptimes[1:(length(steptimes)-1)])
  logp <- dnorm(stepsizes, u, v*timesteps, log = TRUE)
  return(sum(logp))
}
```

```{r}
mu <- 0
v <- seq(.1,10,.005)
logliks <- rep(0,length(v))

for (i in 1:length(v)) {
  logliks[i] <- likRandomly(steptimes, stepsizes, mu, v[i])
}

# MLE from plot
qplot(v, logliks, geom='line')
v[which.max(logliks)]

# MLE from data
timesteps<-c(steptimes[1], steptimes[2:length(steptimes)] - steptimes[1:(length(steptimes)-1)])
sum((stepsizes/timesteps)^2)/length(stepsizes)
```

* Consider the tree in Figure 1 of the Mau et al paper
  \url{http://www.ncbi.nlm.nih.gov/pubmed/11318142} with leafs labelled 1 to 7.
  Let $X()$ be the result of performing standard Brownian motion on the tree,
  starting with $X=0$ at the root of the tree. Let $X^L$ denote the vector of
  values of $X()$ at the seven labelled leafs, and $X^i$ denote the values of
  $X()$ at the six labelled internal nodes, including the root node. So $(X^i,
  X^L)$ has a 13-variate normal distribution. Also $X^i_6=0$ because internal
  node 6 is the root.

  * Write code to simulate the value of $(X^i,X^L)$ given the coalescent times
    $(t_1,\dots,t_6)$ [see Figure].

```{r}
computeTreeRandomly <- function(ct, u=0, v=1) {
  xi <- rep(0, 6)
  xL <- rep(0, 7)

  xi[6] <- 0
  xi[5] <- rnorm(1, u, sqrt(v*ct[6]))
  xi[4] <- xi[5] + rnorm(1, u, sqrt(v*ct[5]))
  xi[3] <- xi[5] + rnorm(1, u, sqrt(v*(ct[5]+ct[4])))
  xi[2] <- xi[3] + rnorm(1, u, sqrt(v*ct[3]))
  xi[1] <- xi[4] + rnorm(1, u, sqrt(v*(ct[3]+ct[2]+ct[4])))

  xL[1] <- xi[4] + rnorm(1, u, sqrt(v*(ct[4]+ct[3]+ct[2]+ct[1])))
  xL[4] <- xi[1] + rnorm(1, u, sqrt(v*(ct[1])))
  xL[7] <- xi[1] + rnorm(1, u, sqrt(v*(ct[1])))
  xL[2] <- xi[3] + rnorm(1, u, sqrt(v*(ct[3]+ct[2]+ct[1])))
  xL[3] <- xi[2] + rnorm(1, u, sqrt(v*(ct[2]+ct[1])))
  xL[6] <- xi[2] + rnorm(1, u, sqrt(v*(ct[2]+ct[1])))
  xL[5] <- rnorm(1, u, sqrt(v*sum(ct)))

  return(list(xi=xi, xL=xL))
}
```

  * Assume from now on that the coalescence times $t_j=1$ for each $j$.
    Compute the covariance matrix for $(X^i,X^L)$. [Hint: you could write
    $(X^i,X^L)=AZ$ for some matrix $A$, where $Z$ is a vector whose elements
    are iid and standard normal; then the covariance is $AA'$.]

```{r}
coalescenttimes <- c(1, 1, 1, 1, 1, 1)
res <- computeTreeRandomly(coalescenttimes)
xi <- res$xi
xL <- res$xL
walk <- c(xi, xL)
```

Each edge in the graph is independent of other edges, so model the tree as a
linear combination of edges.

```{r}
edgelengths <- c(4, 1, 1, 3, 2, 2, 6,
                 3, 1, 1, 2, 1)
# edgelengths <- c(3, 1, 1, 2, 1, 4, 1, 1, 3, 2, 2, 6)
means <- rep(0, length(edgelengths))
z <- rnorm(means, sqrt(edgelengths))

path_xi1 <- c(0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1)
path_xi2 <- c(0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1)
path_xi3 <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1)
path_xi4 <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1)
path_xi5 <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)
path_xi6 <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)

path_xL1 <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1)
path_xL2 <- c(0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1)
path_xL3 <- c(0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1)
path_xL4 <- c(0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1)
path_xL5 <- c(0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0)
path_xL6 <- c(0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1)
path_xL7 <- c(0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1)

A <- matrix(nrow=13, ncol=12, c(path_xi1,
           path_xi2,
           path_xi3,
           path_xi4,
           path_xi5,
           path_xi6,
           path_xL1,
           path_xL2,
           path_xL3,
           path_xL4,
           path_xL5,
           path_xL6,
           path_xL7), byrow=T)

A <- A %*% diag(edgelengths) # this way the linear combo is of standard normals
covar <- A %*% t(A)
covar
```

  * Check that your answer for the covariance and your simulation code roughly
    agree. Specifically, simulate 10,000 times from $(X^i,X^L)$ using your code
    with $t_j=1$, and compare the empirical covariance matrix with your
    computation.

```{r}
nsims <- 10000

runs <- matrix(0, nrow=nsims, ncol=13)

for (i in 1:nsims) {
  res <- computeTreeRandomly(coalescenttimes)
  runs[i,] <- c(res$xi, res$xL)
}
cov(runs)
```

  * Compute the precision matrix (inverse of the covariance matrix) for
    $(X^i,X^L)$. [Although you should never compute the inverse of a matrix,
    you are permitted do so here for pedagogical reasons!] Also find the
    covariance matrix for $X^i$, the covariance matrix for $X^L$ and their
    inverses. Relate the sparsity patterns you do or do not see in the
    precision matrices to what you know about Gaussian Graphical Models.

Precision matrix (total)

```{r}
ginv(covar)
```

Everything is conditionally independent of leaf 5.


Covariance/Precision matrix for internal nodes:

```{r}
covar_i <- covar[1:6, 1:6]
covar_i
ginv(covar_i)
```

Covariance/Precision matrix for leaves:

```{r}
covar_L <- covar[7:13, 7:13]
covar_L
ginv(covar_L)
```

Internal nodes that are more distance from each other on the tree are more
independent. Nodes that are conditionally independent given the position of the
root also have near-0 entries in the precision matrices.


## B: Spatial Gaussian Processes

* Consider the data from
  \url{http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0030339},
  available at https://github.com/stephens999/hgen48600/tree/master/data/CCR5,
  which you can read into R using code from
  https://stephens999.github.io/hgen48600/ccr5.html

```{bash, message=F}
wget -nc https://raw.githubusercontent.com/stephens999/hgen48600/master/data/CCR5/CCR5.freq.txt
```

```{r}
ccr5 <- read.table("CCR5.freq.txt",header=TRUE)
ccr5[,1] <- ifelse(ccr5[,1]>180,ccr5[,1]-360,ccr5[,1]) # changes longitudes>180 to negative

ccr5$count = round(ccr5$Freq* ccr5$SampleSize * 2)
ccr5$fhat = (ccr5$count+1)/(ccr5$SampleSize*2+2)
ccr5$Xhat = log(ccr5$fhat/(1-ccr5$fhat))
```

* These consist of latitude, longitude, and an allele frequency at each
  location. We will model these data as a Gaussian process. Since allele
  frequency lies in [0,1] start by using the transformation
  $x=\log(\hat{f}/(1-\hat{f}))$. (Here $\hat{f}$ is the estimated frequency in
  the code above.) We will let $y$ denote locations in space (latitude,
  longitude) and $x()$ denote the allele frequency varying as a function of
  space, so $x(y)$ is the allele frequency at location $y$. We will model $x()$
  as a Gaussian process, with constant mean $\mu=m$ and squared exponential
  covariance function of the form $a_1 \exp(-(d/a_2)^2)$.

```{r}
ccr5$X <- log(ccr5$Freq/(1-ccr5$Freq))
```

* Hence, $a=(a_1,a_2)$ and the mean $m$ are the parameters to be estimated.

* Write a function to compute the covariance matrix for
  $x^{\text{obs}}:=(x(y_1),\hdots,x(y_r))$ given a value of $a$. Here
  $y_1,\dots,y_r$ are the locations at which you have observations in the
  dataset. Try a few values of $a$ and check that the resulting covariance
  matrix is valid - that is, it is positive semi-definite. (You can use the
  `eigen` function to check the eigenvalues are non-negative).

```{r}
computeCovarianceMatrix <- function(f, a1, a2) {
  d2 <- matrix(0, dim(f)[1], dim(f)[1])
  for (i in 1:dim(d2)[1]) {
    d2[i,] <- (f$Long[i] - f$Long)^2 + (f$Lat[i] - f$Lat)^2
  }
  return(a1*exp( -d2/(a2)^2 ))
  # return(d2)
}
```

```{r}
a1 <- 1
a2 <- 1
covar <- computeCovarianceMatrix(ccr5, a1, a2)
res <- eigen(covar)
sum(res$values < 0) # number of negative eigenvalues
```

```{r}
a1 <- 10
a2 <- 10
covar <- computeCovarianceMatrix(ccr5, a1, a2)
res <- eigen(covar)
sum(res$values < 0) # number of negative eigenvalues
```

```{r}
a1 <- .5
a2 <- .5
covar <- computeCovarianceMatrix(ccr5, a1, a2)
res <- eigen(covar)
sum(res$values < 0) # number of negative eigenvalues
```

* Write a function to compute the log-likelihood for the data $x^\text{obs}$,
  given $a,m$. [Here we assume the mean is constant across the whole region, so
  $m$ is the same at every location].

  * The model here is that $x^{\text{obs}} | m, a \sim N_r(\mu, \Sigma)$ where
    $\Sigma=\Sigma(a)$ is the function of $a$ that you coded above and
    $\mu=rep(m,r)$. So your likelihood just involves computing a multivariate
    normal density. You can use the R function mvtnorm::dmvnorm (with log=TRUE)


```{r}
computeSquaredExponentialLogLikelihood <- function(f, m, covar) {
  mu <- rep(m, dim(f)[1])

}
```


* Try using the R function `optim` (or another approach if you prefer) to
  optimize the likelihood numerically over $a,m$. (I found it seemed to work
  OK, in that it gave similar answers from different starting points, although
  it reported convergence=0; I'm not sure why).

* Now we are going to try deleting each of the observed data points in turn and
  "impute" its value using our model. This process is sometimes known as
  Kriging.

  * Let $X=(X_1,\dots,X_r)$ be $r$-variate normal with mean $\mu$ and variance
    covariance $\Sigma$. Write a function to compute the conditional
    expectation of $X_1$ given $X_2,\dots,X_r$. [This is an application of
    standard results for the conditional mean of a Gaussian from, e.g.
    \url{https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions]}

  * Apply this function to compute $\text{E}(x(y_1) | x(y_2),...,x(y_r))$.
    Notice that this expectation ends up being a weighted linear combination of
    the other datapoints. Intuitively, if allele frequencies vary smoothly in
    space then this weighted linear combination should weight the nearby data
    points more. Does it?

  * Repeat this for each of the $r$ datapoints.

  * How does the accuracy of this imputation scheme compare with just using the
    mean of the other datapoints to impute each datapoint?

## C:
Show that when Francis Galton
(\url{https://en.wikipedia.org/wiki/Francis_Galton}) was comparing children’s
heights to the average of their parents’ heights, he was actually kriging. To
show this, we only need to look at one trio, so you will be working with a 3x3
matrix.

a) Write down the kernel matrix for the two parents and the offspring based on
   their genetic similarity, i.e. using the genetic correlation. Let’s assume
   random mating and approximate the correlation between parents as 0.

b) Predict the height of the child using the kriging (Gaussian process
   prediction) formula.
